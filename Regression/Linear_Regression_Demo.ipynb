{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T18:52:59.329232Z",
     "start_time": "2020-01-07T18:52:59.326085Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up random generator\n",
    "Reproducibility is important when we want to present our works.  \n",
    "In ML, it is really hard to reproduce results, but we can at least try minimizing randomness by fixing seeds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "\n",
    "# This cell should always output the same value!\n",
    "print(\"Test random seed: {}\".format(np.random.uniform()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Linear Regression\n",
    "## Generate Training Data\n",
    "We define the source of training data.  \n",
    "Realistically, we will not have usually access to real data distribution.  Rather, our job is to learn such a model from training data.\n",
    "\n",
    "We model the source as a random variable since there is measurement error while observing the data!  \n",
    "**QUESTION**: What happens if the magnitude of the noise is really high?\n",
    "\n",
    "**NOTE**: A good practice is to always visualize our data first. This gives us knowledge about what to expect when our trained model does not perform as we might have wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T18:53:01.838586Z",
     "start_time": "2020-01-07T18:53:01.832021Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data_1d(x, w=np.array([[3], [2]]), noise_mean=0, noise_variance=1):\n",
    "    \"\"\" This is a function for generating a training set (or maybe test set).\n",
    "    Args:\n",
    "    - x (ndarray (Shape: (N, 1))): A N-column vector corresponding to the inputs.\n",
    "    - w (ndarray (Shape: (2, 1))): A 2-column vector corresponding to the parameters of the linear function.\n",
    "    - noise_mean (float): The mean of a Guassian distribution describing measurement noise.\n",
    "    - noise_variance (float): The variance of a Guassian distribution describing measurement noise.\n",
    "    \n",
    "    Output:\n",
    "    - y (ndarray (Shape: (N, 1))): A N-column vector corresponding to the outputs given the inputs.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = np.random.normal(loc=noise_mean, scale=noise_variance, size=x.shape)\n",
    "    \n",
    "    # Pad with 1's to incorporate the bias term\n",
    "    x = np.hstack((np.ones((num_samples, 1)), x))\n",
    "    \n",
    "    # Compute outputs y, equal to x^T w plus the noise\n",
    "    y = np.matmul(x, w) + noise\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some training data and visual it\n",
    "\n",
    "# 1st and 2nd elements of w are offset and slope (here 3, and 2)\n",
    "true_w = np.array([[3], [2]]) \n",
    "noise_mean = 0\n",
    "noise_variance = 1\n",
    "\n",
    "# Generate training data, for x in range [0 to 9]\n",
    "x = np.expand_dims(np.arange(10), axis=1)\n",
    "y = generate_data_1d(x=x,\n",
    "                     w=true_w,\n",
    "                     noise_mean=noise_mean,\n",
    "                     noise_variance=noise_variance)\n",
    "\n",
    "# Visualize the dataset\n",
    "pprint([(input.item(), output.item()) for input, output in zip(x, y)])\n",
    "\n",
    "fig1 = figure(num=None, figsize=(5, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal parameters, given the training data\n",
    "We now train our linear model based on the training data generated previously.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean/average of x and y.\n",
    "# Recall that x and y are N-column vectors corresponding to N scalar samples\n",
    "mean_x = x.mean()\n",
    "mean_y = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal weight w* given by:\n",
    "# w* = sum_i((y_i - mean_y) * (x_i - mean_x)) / sum_i((x_i - mean_x)^2)\n",
    "w = ((y - mean_y) * (x - mean_x)).sum() / ((x - mean_x) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute optimal bias b* given by:\n",
    "# b* = b* = mean_y - w* * mean_x\n",
    "b = mean_y - w * mean_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the weight and bias into one vector for convenience\n",
    "# Also want to check whether our solutions make sense!\n",
    "w = np.array([[b], [w]])\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T18:53:09.290979Z",
     "start_time": "2020-01-07T18:53:09.185680Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_lines(x, observed_y, trained_w, true_w):\n",
    "    \"\"\" This visualizes data with the true and estimated lines \"\"\"\n",
    "    fig = figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.scatter(x, observed_y)\n",
    "    \n",
    "    pad_x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    plt.plot(x, pad_x @ trained_w, label=\"Estimated Parameters\")\n",
    "    plt.plot(x, pad_x @ true_w, label=\"Ground Truth Parameters\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_lines(x, y, w, true_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the trained parameters are not the same as the ground truth parameters!  \n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Linear Regression\n",
    "We can modify `generate_data` and generalize to multi-dimensional case.  \n",
    "For the example below, we will try the 2D case since we can still visualize things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_nd(x, w, noise_mean=0, noise_variance=1):\n",
    "    \"\"\" This function generates noisy training set (or maybe a test set).\n",
    "    Args:\n",
    "    - x (ndarray (Shape: (N, D))): A NxD matrix corresponding to the inputs. Each row correspond to an input\n",
    "    - w (ndarray (Shape: (D + 1, 1))): A (D + 1)-column vector corresponding to the parameters of the linear function.\n",
    "    - noise_mean (float): The mean of a Guassian distribution describing measurement noise.\n",
    "    - noise_variance (float): The variance of a Guassian distribution describing measurement noise.\n",
    "    \n",
    "    Output:\n",
    "    - y (ndarray (Shape: (N, 1))): A N-column vector corresponding to the outputs given the inputs.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # Generate noise\n",
    "    noise = np.random.normal(loc=noise_mean, scale=noise_variance, size=(num_samples, 1))\n",
    "    \n",
    "    # Pad 1's for the bias term\n",
    "    x = np.hstack((np.ones((num_samples, 1)), x))\n",
    "    # Compute outputs y\n",
    "    y = np.matmul(x, w) + noise\n",
    "    \n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data \n",
    "true_w = np.array([[3], [2], [1]])\n",
    "noise_mean = 0\n",
    "noise_variance = 5\n",
    "\n",
    "# Generate training data\n",
    "x_1, x_2 = np.meshgrid(np.arange(10), np.arange(10))\n",
    "x = np.vstack((x_1.flatten(), x_2.flatten())).T\n",
    "y = generate_data_nd(x=x,\n",
    "                     w=true_w,\n",
    "                     noise_mean=noise_mean,\n",
    "                     noise_variance=noise_variance)\n",
    "\n",
    "# Visualize the dataset\n",
    "pprint([(list(input), output.item()) for input, output in zip(x, y)][:20])\n",
    "\n",
    "fig = figure(num=None, figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(x[:, 0], x[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-07T18:53:06.342175Z",
     "start_time": "2020-01-07T18:53:06.335057Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_optimal_parameters_nd(x, y):\n",
    "    \"\"\" Compute closed form solution for linear regression!\n",
    "    Optimal weight w* in linear regression is given by w* = (X^T X)^(-1) X^T Y\n",
    "    \n",
    "    Args:\n",
    "    - x (ndarray (Shape: (N, D))): A NxD matrix corresponding to the inputs.\n",
    "    - y (ndarray (Shape: (N, 1))): A N-column vector corresponding to the outputs given the inputs.\n",
    "    \n",
    "    Output:\n",
    "    - w (ndarray (Shape: (2, 1))): A 2-column vector corresponding to the bias and weight of the linear model.\n",
    "                                   w = [[bias], [weight]]\n",
    "    \"\"\"\n",
    "    # Pad 1's for the bias term, Why?\n",
    "    x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "\n",
    "    # Note that we could use pseudoinverse here instead: np.linalg.pinv\n",
    "    # @ is alias for matmul\n",
    "    p1 = np.linalg.inv(x.T @ x) # (X^T X) inverse\n",
    "    p2 = x.T @ y # X^T Y\n",
    "    w = p1 @ p2\n",
    "    return w\n",
    "\n",
    "w = find_optimal_parameters_nd(x, y)\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_planes(x, observed_y, trained_w, true_w, grid_shape=(10, 10)):\n",
    "    \"\"\" This visualize the 3D curves \"\"\"\n",
    "    pad_x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "    trained_y = (pad_x @ trained_w).reshape(grid_shape)\n",
    "    true_y = (pad_x @ true_w).reshape(grid_shape)\n",
    "    \n",
    "    fig = figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x[:, 0], x[:, 1], observed_y)\n",
    "    x_1 = x[:, 0].reshape(grid_shape)\n",
    "    x_2 = x[:, 1].reshape(grid_shape)\n",
    "    ax.plot_surface(x_1, x_2, trained_y, alpha=0.5, color=\"blue\")\n",
    "    ax.plot_surface(x_1, x_2, true_y, alpha=0.5, color=\"red\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_planes(x, y, w, true_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as expected, the estimated parameters are close to, but not the same as the ground truth parameters!  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
